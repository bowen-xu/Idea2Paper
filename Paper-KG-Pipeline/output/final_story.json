{
  "title": "NoWag: Unifying Vector Quantization and Pruning via Generative Trajectory Diffusion for One-Shot Shape-Preserving Compression",
  "abstract": "We introduce NoWag, a unified framework designed to achieve one-shot shape-preserving compression that seamlessly integrates vector quantization and pruning paradigms. Unlike traditional methods that view compression as a lossy reduction of static values, NoWag reframes the process as a generative structural completion task. Our approach extracts a sparse 'skeletal geometry' using a unified optimization strategy and leverages a trajectory-informed estimation prior to guide a conditional diffusion model. This model 'hallucinates' the high-fidelity weight texture required to preserve the network's functional manifold, effectively replacing iterative retraining with a single-step generative reconstruction. By transforming the compressed model into a generative seed, NoWag maintains the intrinsic shape and performance of neural networks while significantly reducing computational overhead.",
  "problem_framing": "We reframe the challenge of neural network compression from a lossy reduction of static parameters to a generative structural completion task. Instead of viewing compression merely as discarding information through isolated pruning and quantization, we position it as the extraction of a high-dimensional latent skeleton that implies a dense, high-precision distribution. The problem shifts from minimizing storage error to maximizing the recoverability of the network's functional geometry through generative priors, demanding a framework that treats the compressed structure as a seed for synthesis rather than a degraded artifact.",
  "gap_pattern": "Existing methods fail to achieve efficient one-shot compression because they treat vector quantization and pruning as competing, destructive operations requiring separate, disjoint optimization pipelines. They rely heavily on post-hoc fine-tuning to recover accuracy, operating under the assumption that structure is static and information is permanently lost. This ignores the inherent generative manifold where a specific sparse structure implies a dense, high-precision distribution, leading to high computational costs and a fundamental inability to preserve the model's functional shape without expensive iterative corrections.",
  "solution": "NoWag addresses these limitations by introducing a unified framework where structural optimization and functional synthesis co-evolve. We leverage a dual-step optimization mechanism to define a rigid 'skeletal geometry' via simultaneous pruning and quantization, while capturing a 'trajectory prior' from historical training data. This trajectory serves as a conditioning vector for a conditional diffusion model, which 'hallucinates' the high-fidelity weight texture required to preserve functional geometry. This method bridges the gap between aggressive compression and model integrity by replacing the standard weight-retraining phase with a one-step generative reconstruction, transforming the compressed model from a static artifact into a generative seed.",
  "method_skeleton": "Step 1: Implement a stochastic extragradient-type algorithm using two stepsizes, where a fixed stepsize governs binary mask updates for pruning and a diminishing stepsize refines the quantization codebooks to establish the structural skeleton; Step 2: Develop a two-layer feed-forward nonparametric estimation module where the first layer learns univariate basis functions for weight importance and the second layer learns a single index function for quantization levels, providing the feature conditioning for the generative process; Step 3: Develop a trajectory-informed derivative estimation method that utilizes the history of function queries to construct a conditioning prior for a diffusion model, enabling dynamic virtual updates that synthesize the final high-fidelity weights in a single shot without standard backpropagation.",
  "innovation_claims": [
    "Transform the ontology of compressed models from degraded static artifacts into generative seeds by unifying NoWag's structural biases with trajectory-based diffusion priors, enabling recovery of functional geometry from sparse skeletons.",
    "Reframe weight recovery from iterative error correction to a one-shot structural completion task by utilizing the compression trajectory history as the denoising schedule for generative restoration.",
    "Unify vector quantization and pruning not as separate reduction steps but as co-dependent components of a latent code generation process, effectively decoupling the 'structural skeleton' from the 'functional texture' to eliminate the need for costly retraining."
  ],
  "experiments_plan": "We evaluate NoWag on large-scale vision benchmarks (ImageNet-1K) and language tasks (WikiText-103) using diverse architectures including ResNet-50, ViT-B/16, and GPT-2. We compare against state-of-the-art compression methods such as AMC, DCQ, and ZeroQ, measuring accuracy retention, compression ratio, and FLOPs reduction. Ablation studies will verify the contribution of the trajectory-informed prior versus random initialization and analyze the computational complexity of the one-shot generative reconstruction compared to standard fine-tuning pipelines."
}