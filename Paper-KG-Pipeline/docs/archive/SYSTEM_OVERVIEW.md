# Idea2Pattern çŸ¥è¯†å›¾è°±å¬å›ç³»ç»Ÿ - å®Œæ•´è¯´æ˜

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [æ•°æ®æºä¸èŠ‚ç‚¹æ„å»º](#æ•°æ®æºä¸èŠ‚ç‚¹æ„å»º)
3. [çŸ¥è¯†å›¾è°±è¾¹æ„å»º](#çŸ¥è¯†å›¾è°±è¾¹æ„å»º)
4. [ä¸‰è·¯å¬å›ç­–ç•¥](#ä¸‰è·¯å¬å›ç­–ç•¥)
5. [å¤šè·¯èåˆä¸ç²¾æ’](#å¤šè·¯èåˆä¸ç²¾æ’)
6. [å½“å‰å±€é™ä¸æ”¹è¿›æ–¹å‘](#å½“å‰å±€é™ä¸æ”¹è¿›æ–¹å‘)

---

## ç³»ç»Ÿæ¦‚è¿°

### æ ¸å¿ƒç›®æ ‡
**è¾“å…¥**: ç”¨æˆ·çš„ç ”ç©¶ Idea æè¿°
**è¾“å‡º**: Top-10 æœ€ç›¸å…³çš„ç ”ç©¶ Pattern (å†™ä½œå¥—è·¯/æ–¹æ³•æ¨¡æ¿)

### æŠ€æœ¯æ¶æ„
```
ç”¨æˆ· Idea
    â†“
çŸ¥è¯†å›¾è°± (16,790 èŠ‚ç‚¹, 444,872 æ¡è¾¹)
    â†“
ä¸‰è·¯å¹¶è¡Œå¬å›
    â”œâ”€ è·¯å¾„1: ç›¸ä¼¼ Idea â†’ Pattern (æƒé‡ 0.4)
    â”œâ”€ è·¯å¾„2: ç›¸å…³ Domain â†’ Pattern (æƒé‡ 0.2)
    â””â”€ è·¯å¾„3: ç›¸ä¼¼ Paper â†’ Pattern (æƒé‡ 0.4)
    â†“
åŠ æƒèåˆ + ç²¾æ’
    â†“
Top-10 Pattern æ¨è
```

### æ•°æ®è§„æ¨¡ (V3ç‰ˆæœ¬)
- **æ•°æ®æº**: ICLR 2025 è®ºæ–‡æ•°æ®é›†
- **Idea èŠ‚ç‚¹**: 8,284 ä¸ª (æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæƒ³æ³•)
- **Pattern èŠ‚ç‚¹**: 124 ä¸ª (åŸºäºèšç±»çš„å†™ä½œå¥—è·¯)
- **Domain èŠ‚ç‚¹**: 98 ä¸ª (ç ”ç©¶é¢†åŸŸ)
- **Paper èŠ‚ç‚¹**: 8,285 ç¯‡è®ºæ–‡
- **æ€»è¾¹æ•°**: 444,872 æ¡

---

## æ•°æ®æºä¸èŠ‚ç‚¹æ„å»º

### 1. æ•°æ®æº

#### è¾“å…¥æ–‡ä»¶
1. **`assignments.jsonl`** - è®ºæ–‡åˆ†é…ä¿¡æ¯
   - `paper_id`: è®ºæ–‡å”¯ä¸€æ ‡è¯†
   - `paper_title`: è®ºæ–‡æ ‡é¢˜
   - `domain`: ä¸»é¢†åŸŸ (å¦‚ "Natural Language Processing")
   - `sub_domains`: å­é¢†åŸŸåˆ—è¡¨ (å¦‚ ["Text Classification", "Transformers"])
   - `cluster_id`: èšç±»ID (å¯¹åº” Pattern)
   - `global_pattern_id`: å…¨å±€ Pattern ID

2. **`clusters.jsonl`** - Pattern èšç±»ä¿¡æ¯
   - `cluster_id`: èšç±»ID
   - `size`: èšç±»å¤§å° (åŒ…å«å¤šå°‘ç¯‡è®ºæ–‡)
   - `summary`: Pattern æ‘˜è¦ä¿¡æ¯ (ç¤ºä¾‹ã€æŠ€å·§ç­‰)

3. **`pattern_details.jsonl`** - è®ºæ–‡è¯¦ç»† Pattern
   - `paper_id`: è®ºæ–‡ID
   - `idea`: æ ¸å¿ƒæƒ³æ³•æè¿° (å­—ç¬¦ä¸²)
   - `research_patterns`: ç ”ç©¶æ¨¡å¼è¯¦æƒ… (base_problem, solution_pattern, story ç­‰)

4. **`iclr_patterns_full_cn_912.jsonl`** - LLM å¢å¼ºçš„ Pattern æ€»ç»“
   - `cluster_id`: èšç±»ID
   - `representative_ideas`: LLM ç”Ÿæˆçš„å½’çº³æ€»ç»“
   - `common_tricks`: å¸¸è§æŠ€å·§
   - `naming_suggestion`: Pattern å‘½åå»ºè®®

### 2. èŠ‚ç‚¹æ„å»ºæµç¨‹

#### Pattern èŠ‚ç‚¹ (124ä¸ª)
**æ¥æº**: `clusters.jsonl` + LLM å¢å¼º

**æ„å»ºé€»è¾‘**:
```python
for cluster in clusters:
    pattern_node = {
        'pattern_id': f"pattern_{cluster_id}",
        'name': llm_summary['naming_suggestion'],  # LLMç”Ÿæˆçš„åç§°
        'size': cluster['size'],  # è®ºæ–‡æ•°é‡
        'summary': cluster['summary'],  # åŸå§‹æ‘˜è¦
        'llm_enhanced_summary': {  # LLMå¢å¼º
            'representative_ideas': "...",
            'common_tricks': [...],
            'application_scenarios': [...]
        }
    }
```

**å…³é”®å­—æ®µ**:
- `pattern_id`: å¦‚ `pattern_5`
- `name`: å¦‚ "Reframing Zero-Shot Generalization"
- `size`: èšç±»å¤§å°,åæ˜ è¯¥ Pattern çš„æµè¡Œåº¦
- `llm_enhanced`: æ˜¯å¦ç»è¿‡ LLM å¢å¼º (912/124 å·²å¢å¼º)

---

#### Idea èŠ‚ç‚¹ (8,284ä¸ª)
**æ¥æº**: `pattern_details.jsonl`

**æ„å»ºé€»è¾‘**:
```python
for paper in pattern_details:
    idea_node = {
        'idea_id': f"idea_{index}",
        'description': paper['idea'],  # æ ¸å¿ƒæƒ³æ³•(å­—ç¬¦ä¸²)
        'source_paper_ids': [paper['paper_id']],
        'pattern_ids': []  # åç»­å…³è”å¡«å……
    }
```

**å…³é”®å­—æ®µ**:
- `idea_id`: å¦‚ `idea_42`
- `description`: æ ¸å¿ƒæƒ³æ³•æè¿° (å¹³å‡é•¿åº¦ 150-300 å­—ç¬¦)
- `pattern_ids`: è¯¥ Idea ä½¿ç”¨çš„ Pattern åˆ—è¡¨ (å…³è”åå¡«å……)

**ç‰¹ç‚¹**:
- æ¯ç¯‡è®ºæ–‡å¯¹åº”ä¸€ä¸ª Idea èŠ‚ç‚¹
- V3 ç‰ˆæœ¬ä¸­ `idea` æ˜¯ç®€å•å­—ç¬¦ä¸²,è€ŒéåµŒå¥—å­—å…¸

---

#### Domain èŠ‚ç‚¹ (98ä¸ª)
**æ¥æº**: `assignments.jsonl` (èšåˆ)

**æ„å»ºé€»è¾‘**:
```python
domain_stats = defaultdict(lambda: {
    'paper_count': 0,
    'sub_domains': set(),
    'related_patterns': set()
})

for assignment in assignments:
    domain = assignment['domain']
    domain_stats[domain]['paper_count'] += 1
    domain_stats[domain]['sub_domains'].update(assignment['sub_domains'])
    domain_stats[domain]['related_patterns'].add(f"pattern_{cluster_id}")

# ç”Ÿæˆ Domain èŠ‚ç‚¹
for domain_name, stats in domain_stats.items():
    domain_node = {
        'domain_id': f"domain_{index}",
        'name': domain_name,
        'paper_count': stats['paper_count'],
        'sub_domains': sorted(list(stats['sub_domains'])),
        'related_pattern_ids': sorted(list(stats['related_patterns']))
    }
```

**å…³é”®å­—æ®µ**:
- `domain_id`: å¦‚ `domain_0`
- `name`: å¦‚ "Natural Language Processing"
- `paper_count`: è¯¥é¢†åŸŸçš„è®ºæ–‡æ•°é‡
- `sub_domains`: å­é¢†åŸŸåˆ—è¡¨ (é€šå¸¸ 10-50 ä¸ª)
- `related_pattern_ids`: è¯¥é¢†åŸŸç›¸å…³çš„ Pattern åˆ—è¡¨

**ç¤ºä¾‹**:
```json
{
  "domain_id": "domain_1",
  "name": "Computer Vision",
  "paper_count": 1076,
  "sub_domains": ["3D Reconstruction", "Object Detection", "Image Synthesis", ...],
  "related_pattern_ids": ["pattern_10", "pattern_25", ...]
}
```

---

#### Paper èŠ‚ç‚¹ (8,285ä¸ª)
**æ¥æº**: `assignments.jsonl` + `pattern_details.jsonl` (åˆå¹¶)

**æ„å»ºé€»è¾‘**:
```python
for assignment in assignments:
    paper_id = assignment['paper_id']
    details = pattern_details.get(paper_id, {})

    paper_node = {
        'paper_id': paper_id,
        'title': assignment['paper_title'],
        'domain': assignment['domain'],
        'sub_domains': assignment['sub_domains'],
        'idea': details.get('idea', ''),  # å­—ç¬¦ä¸²
        'global_pattern_id': assignment['global_pattern_id'],
        'cluster_id': assignment['cluster_id'],

        # åç»­å…³è”å¡«å……
        'pattern_id': '',
        'idea_id': '',
        'domain_id': ''
    }
```

**å…³é”®å­—æ®µ**:
- `paper_id`: å¦‚ `Kn-HA8DFik` (ICLR è®ºæ–‡ ID)
- `title`: è®ºæ–‡æ ‡é¢˜
- `idea`: æ ¸å¿ƒæƒ³æ³•æè¿° (å­—ç¬¦ä¸²,ç”¨äºç›¸ä¼¼åº¦è®¡ç®—)
- `pattern_id`: å…³è”çš„ Pattern
- `idea_id`: å…³è”çš„ Idea
- `domain_id`: å…³è”çš„ Domain

**å½“å‰ç¼ºå¤±å­—æ®µ**:
- `reviews`: âš ï¸ æš‚æ—  Review æ•°æ®,è´¨é‡åˆ†é»˜è®¤ 0.5

---

## çŸ¥è¯†å›¾è°±è¾¹æ„å»º

### è¾¹çš„åˆ†ç±»

| è¾¹ç±»å‹ | ç”¨é€” | æ•°é‡ |
|--------|------|------|
| **åŸºç¡€è¿æ¥è¾¹** | å»ºç«‹å®ä½“é—´åŸºæœ¬å…³ç³» | ~25,000 |
| **å¬å›è¾…åŠ©è¾¹** | æ”¯æŒä¸‰è·¯å¬å›ç­–ç•¥ | ~420,000 |

---

### 1. åŸºç¡€è¿æ¥è¾¹

#### (1) Paper â†’ Idea (`implements`)
**ç”¨é€”**: è¡¨ç¤ºè®ºæ–‡å®ç°äº†æŸä¸ªæ ¸å¿ƒ Idea

**æ„å»ºé€»è¾‘**:
```python
for paper in papers:
    G.add_edge(
        paper['paper_id'],
        paper['idea_id'],
        relation='implements'
    )
```

**æƒé‡**: æ— æƒé‡ (å¸ƒå°”å…³ç³»)

---

#### (2) Paper â†’ Pattern (`uses_pattern`)
**ç”¨é€”**: è¡¨ç¤ºè®ºæ–‡ä½¿ç”¨äº†æŸä¸ªå†™ä½œ Pattern

**æ„å»ºé€»è¾‘**:
```python
for paper in papers:
    paper_quality = _get_paper_quality(paper)  # åŸºäº review è¯„åˆ†

    G.add_edge(
        paper['paper_id'],
        paper['pattern_id'],
        relation='uses_pattern',
        quality=paper_quality  # [0, 1]
    )
```

**æƒé‡**:
- `quality`: Paper çš„ç»¼åˆè´¨é‡åˆ†æ•° (0-1)
  - **æœ‰ Review æ•°æ®æ—¶**: `(avg_review_score - 1) / 9`
  - **æ—  Review æ•°æ®æ—¶** (V3 å½“å‰çŠ¶æ€): `0.5` (é»˜è®¤å€¼)

**ç¤ºä¾‹**:
```json
{
  "source": "Kn-HA8DFik",
  "target": "pattern_5",
  "relation": "uses_pattern",
  "quality": 0.5
}
```

---

#### (3) Paper â†’ Domain (`in_domain`)
**ç”¨é€”**: è¡¨ç¤ºè®ºæ–‡å±äºæŸä¸ªç ”ç©¶é¢†åŸŸ

**æ„å»ºé€»è¾‘**:
```python
for paper in papers:
    G.add_edge(
        paper['paper_id'],
        paper['domain_id'],
        relation='in_domain'
    )
```

**æƒé‡**: æ— æƒé‡ (å¸ƒå°”å…³ç³»)

---

### 2. å¬å›è¾…åŠ©è¾¹

#### (1) Idea â†’ Domain (`belongs_to`)
**ç”¨é€”**: æ”¯æŒè·¯å¾„2å¬å› (é¢†åŸŸç›¸å…³æ€§å¬å›)

**æ„å»ºé€»è¾‘**:
```python
for idea in ideas:
    domain_counts = defaultdict(int)

    # ç»Ÿè®¡ Idea ç›¸å…³ Paper åœ¨å„ Domain çš„åˆ†å¸ƒ
    for paper_id in idea['source_paper_ids']:
        paper = paper_id_to_paper[paper_id]
        domain_counts[paper['domain_id']] += 1

    # åˆ›å»ºè¾¹,æƒé‡ä¸ºå æ¯”
    total_papers = len(idea['source_paper_ids'])
    for domain_id, count in domain_counts.items():
        weight = count / total_papers  # å æ¯”ä½œä¸ºæƒé‡

        G.add_edge(
            idea['idea_id'],
            domain_id,
            relation='belongs_to',
            weight=weight,  # [0, 1]
            paper_count=count
        )
```

**æƒé‡**:
- `weight`: Idea ç›¸å…³ Paper åœ¨è¯¥ Domain çš„å æ¯” (0-1)

**ç¤ºä¾‹**:
```json
{
  "source": "idea_42",
  "target": "domain_2",
  "relation": "belongs_to",
  "weight": 0.75,
  "paper_count": 3,
  "total_papers": 4
}
```

---

#### (2) Pattern â†’ Domain (`works_well_in`)
**ç”¨é€”**: æ”¯æŒè·¯å¾„2å¬å›,è¡¨ç¤º Pattern åœ¨æŸ Domain çš„æ•ˆæœ

**æ„å»ºé€»è¾‘**:
```python
for pattern in patterns:
    # æŒ‰ Domain åˆ†ç»„ç»Ÿè®¡ Pattern çš„ä½¿ç”¨æƒ…å†µ
    domain_papers = defaultdict(list)

    for paper_id in pattern['sample_paper_ids']:
        paper = paper_id_to_paper[paper_id]
        domain_papers[paper['domain_id']].append(paper)

    # ä¸ºæ¯ä¸ª Domain è®¡ç®—æ•ˆæœæŒ‡æ ‡
    for domain_id, papers in domain_papers.items():
        # è®¡ç®— Pattern åœ¨è¯¥ Domain çš„å¹³å‡è´¨é‡
        qualities = [_get_paper_quality(p) for p in papers]
        avg_quality = np.mean(qualities)

        # è®¡ç®—è¯¥ Domain çš„åŸºçº¿è´¨é‡
        all_domain_papers = get_papers_in_domain(domain_id)
        domain_baseline = np.mean([_get_paper_quality(p) for p in all_domain_papers])

        # æ•ˆæœå¢ç›Š = Patternå¹³å‡è´¨é‡ - DomainåŸºçº¿
        effectiveness = avg_quality - domain_baseline  # [-1, 1]

        # ç½®ä¿¡åº¦ = åŸºäºæ ·æœ¬æ•°
        frequency = len(papers)
        confidence = min(frequency / 20, 1.0)  # [0, 1]

        G.add_edge(
            pattern['pattern_id'],
            domain_id,
            relation='works_well_in',
            frequency=frequency,
            effectiveness=effectiveness,
            confidence=confidence,
            avg_quality=avg_quality,
            baseline=domain_baseline
        )
```

**æƒé‡**:
- `effectiveness`: Pattern åœ¨è¯¥ Domain çš„æ•ˆæœå¢ç›Š (ç›¸å¯¹åŸºçº¿) [-1, 1]
  - **æ­£å€¼**: Pattern åœ¨è¯¥ Domain æ•ˆæœå¥½äºå¹³å‡æ°´å¹³
  - **è´Ÿå€¼**: Pattern åœ¨è¯¥ Domain æ•ˆæœä½äºå¹³å‡æ°´å¹³
- `confidence`: åŸºäºæ ·æœ¬æ•°çš„ç½®ä¿¡åº¦ [0, 1]
  - æ ·æœ¬æ•° â‰¥ 20 æ—¶,ç½®ä¿¡åº¦è¾¾åˆ° 1.0
  - æ ·æœ¬æ•°è¶Šå°‘,ç½®ä¿¡åº¦è¶Šä½

**ç¤ºä¾‹**:
```json
{
  "source": "pattern_5",
  "target": "domain_2",
  "relation": "works_well_in",
  "frequency": 15,
  "effectiveness": 0.12,
  "confidence": 0.75,
  "avg_quality": 0.82,
  "baseline": 0.70
}
```

---

#### (3) Idea â†’ Paper (`similar_to_paper`)
**ç”¨é€”**: æ”¯æŒè·¯å¾„3å¬å› (ç›¸ä¼¼ Paper å¬å›)

**æ³¨æ„**: è¯¥è¾¹åœ¨å½“å‰ç‰ˆæœ¬(V3.1)ä¸­**å·²é¢„æ„å»ºä½†æœªç›´æ¥ä½¿ç”¨**ã€‚è·¯å¾„3å¬å›æ”¹ä¸º**å®æ—¶è®¡ç®—**ç”¨æˆ·Ideaä¸Paper Titleçš„ç›¸ä¼¼åº¦,ä»¥é¿å…ä¸è·¯å¾„1é‡å¤ã€‚

**è¾¹æ„å»ºé€»è¾‘** (ä¿ç•™ç”¨äºæœªæ¥æ‰©å±•):
```python
for idea in ideas:
    similarities = []

    # è®¡ç®—ä¸æ‰€æœ‰ Paper çš„ç›¸ä¼¼åº¦
    for paper in papers:
        similarity = compute_text_similarity(
            idea['description'],
            paper['idea']
        )

        if similarity < 0.1:  # è¿‡æ»¤ä½ç›¸ä¼¼åº¦
            continue

        paper_quality = _get_paper_quality(paper)
        combined_weight = similarity * paper_quality

        similarities.append({
            'paper_id': paper['paper_id'],
            'similarity': similarity,
            'quality': paper_quality,
            'combined_weight': combined_weight
        })

    # æ’åºå¹¶åªä¿ç•™ Top-50 (é¿å…è¾¹è¿‡å¤š)
    similarities.sort(key=lambda x: x['combined_weight'], reverse=True)

    for item in similarities[:50]:
        G.add_edge(
            idea['idea_id'],
            item['paper_id'],
            relation='similar_to_paper',
            similarity=item['similarity'],
            quality=item['quality'],
            combined_weight=item['combined_weight']
        )
```

**æƒé‡**:
- `similarity`: Idea ä¸ Paper çš„è¯­ä¹‰ç›¸ä¼¼åº¦ [0, 1]
- `quality`: Paper è´¨é‡åˆ†æ•° [0, 1]
- `combined_weight`: `similarity Ã— quality` [0, 1]

**è·¯å¾„3å®é™…å¬å›**:
- ä¸ä½¿ç”¨é¢„æ„å»ºçš„è¾¹,è€Œæ˜¯å®æ—¶è®¡ç®—ç”¨æˆ·Ideaä¸Paper **Title**çš„ç›¸ä¼¼åº¦
- è¿™æ ·ç¡®ä¿è·¯å¾„1(åŸºäºIdea Description)å’Œè·¯å¾„3(åŸºäºPaper Title)äº’è¡¥

---

## ä¸‰è·¯å¬å›ç­–ç•¥

### è®¾è®¡ç†å¿µ: ä¸‰è·¯äº’è¡¥

ä¸‰è·¯å¬å›ä»ä¸åŒç»´åº¦æ•æ‰ç”¨æˆ·éœ€æ±‚,é¿å…é‡å¤å’Œä¿¡æ¯å†—ä½™:

| è·¯å¾„ | åŒ¹é…å¯¹è±¡ | æ•æ‰ç»´åº¦ | æƒé‡ | å…¸å‹åœºæ™¯ |
|------|---------|---------|------|---------|
| **è·¯å¾„1** | Idea Description | **æ ¸å¿ƒæ€æƒ³/æ¦‚å¿µ**ç›¸ä¼¼æ€§ | 0.4 | ç”¨æˆ·æè¿°ä¸å†å²æˆåŠŸæ¡ˆä¾‹çš„æ ¸å¿ƒæ€è·¯ä¸€è‡´ |
| **è·¯å¾„2** | Domain & Sub-domains | **é¢†åŸŸæ³›åŒ–**èƒ½åŠ› | 0.2 | ç”¨æˆ·Ideaå±äºæŸé¢†åŸŸ,è¯¥é¢†åŸŸæœ‰éªŒè¯æœ‰æ•ˆçš„Pattern |
| **è·¯å¾„3** | Paper Title | **ç ”ç©¶ä¸»é¢˜/å…·ä½“é—®é¢˜**ç›¸ä¼¼æ€§ | 0.4 | ç”¨æˆ·æƒ³è§£å†³çš„å…·ä½“é—®é¢˜ä¸æŸäº›è®ºæ–‡æ ‡é¢˜è¡¨è¿°ç±»ä¼¼ |

**äº’è¡¥æ€§è¯´æ˜**:
- **è·¯å¾„1 vs è·¯å¾„3**:
  - è·¯å¾„1å…³æ³¨"æƒ³æ³•æœ¬è´¨"(å¦‚ "ä½¿ç”¨è’¸é¦æå‡æ¨¡å‹æ•ˆç‡")
  - è·¯å¾„3å…³æ³¨"ç ”ç©¶æ–¹å‘"(å¦‚ "Cross-Domain Text Classification with Transformers")
  - å³ä½¿Ideaç›¸åŒ,è®ºæ–‡æ ‡é¢˜å¯èƒ½èšç„¦ä¸åŒåº”ç”¨åœºæ™¯
- **è·¯å¾„2çš„æ³›åŒ–ä½œç”¨**: å³ä½¿ç”¨æˆ·Ideaæ˜¯å…¨æ–°çš„,åªè¦å±äºæŸä¸ªæˆç†Ÿé¢†åŸŸ,ä¹Ÿèƒ½å¬å›è¯¥é¢†åŸŸé€šç”¨çš„æœ‰æ•ˆPattern

---

### è·¯å¾„1: ç›¸ä¼¼ Idea å¬å› (Idea â†’ Idea â†’ Pattern)

#### å¬å›æµç¨‹
```
ç”¨æˆ· Idea (æ–‡æœ¬)
    â†“ [ç²—æ’] Jaccard ç­›é€‰ Top-100
å€™é€‰ Idea (100ä¸ª)
    â†“ [ç²¾æ’] Embedding é‡æ’ Top-10
ç›¸ä¼¼ Idea (10ä¸ª)
    â†“ ç›´æ¥è·å– idea.pattern_ids
Pattern é›†åˆ
    â†“ æŒ‰ç›¸ä¼¼åº¦åŠ æƒç´¯åŠ 
Top-10 Pattern (å¾—åˆ†å­—å…¸)
```

#### ä¸¤é˜¶æ®µå¬å›ä¼˜åŒ–

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤é˜¶æ®µ?**
- å…¨é‡ Embedding æ£€ç´¢: 8,284 æ¬¡ API è°ƒç”¨,è€—æ—¶ **~7 åˆ†é’Ÿ** âŒ
- ä¸¤é˜¶æ®µå¬å›: 100 æ¬¡ API è°ƒç”¨,è€—æ—¶ **~10 ç§’** âœ… (æé€Ÿ 40 å€)

**ç²—æ’é˜¶æ®µ** (Jaccard):
```python
# å¯¹æ‰€æœ‰ Idea å¿«é€Ÿè®¡ç®— Jaccard ç›¸ä¼¼åº¦
coarse_similarities = []
for idea in ideas:  # 8,284 ä¸ª
    sim = compute_jaccard_similarity(user_idea, idea['description'])
    if sim > 0:
        coarse_similarities.append((idea_id, sim))

# æ’åºå¹¶å– Top-100
coarse_similarities.sort(reverse=True)
candidates = coarse_similarities[:100]
```

**ç²¾æ’é˜¶æ®µ** (Embedding):
```python
# å¯¹å€™é€‰ Idea ä½¿ç”¨ Embedding é‡æ–°è®¡ç®—
fine_similarities = []
for idea_id, _ in candidates:  # 100 ä¸ª
    idea = idea_id_to_idea[idea_id]
    sim = compute_embedding_similarity(user_idea, idea['description'])
    if sim > 0:
        fine_similarities.append((idea_id, sim))

# æ’åºå¹¶å– Top-10
fine_similarities.sort(reverse=True)
top_ideas = fine_similarities[:10]
```

**Embedding API**:
- ä½¿ç”¨ Qwen3-Embedding-4B æ¨¡å‹
- è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: `cosine_sim = dot(emb1, emb2) / (norm(emb1) * norm(emb2))`

---

#### Pattern å¾—åˆ†è®¡ç®—

**ç®—åˆ†é€»è¾‘**:
```python
pattern_scores = defaultdict(float)

for idea_id, similarity in top_10_ideas:
    idea = idea_id_to_idea[idea_id]

    # V3 ç‰ˆæœ¬: ç›´æ¥ä» Idea èŠ‚ç‚¹è·å– pattern_ids
    for pattern_id in idea['pattern_ids']:
        # å¾—åˆ† = ç›¸ä¼¼åº¦ (å¤šä¸ª Idea ä½¿ç”¨åŒä¸€ Pattern æ—¶ä¼šç´¯åŠ )
        pattern_scores[pattern_id] += similarity

# æ’åºå¹¶åªä¿ç•™ Top-10
sorted_patterns = sorted(pattern_scores.items(), reverse=True)
top_patterns = dict(sorted_patterns[:10])
```

**å…³é”®ç‚¹**:
- å¦‚æœå¤šä¸ªç›¸ä¼¼ Idea éƒ½ä½¿ç”¨äº†åŒä¸€ä¸ª Pattern,å¾—åˆ†ä¼š**ç´¯åŠ **
- æœ€ç»ˆåªä¿ç•™å¾—åˆ†æœ€é«˜çš„ **Top-10 ä¸ª Pattern**

**ç¤ºä¾‹**:
```
ç”¨æˆ· Idea: "ä½¿ç”¨ Transformer è¿›è¡Œæ–‡æœ¬åˆ†ç±»"

ç›¸ä¼¼ Idea_1 (ç›¸ä¼¼åº¦ 0.8) â†’ [pattern_5, pattern_10]
ç›¸ä¼¼ Idea_2 (ç›¸ä¼¼åº¦ 0.7) â†’ [pattern_5, pattern_20]
ç›¸ä¼¼ Idea_3 (ç›¸ä¼¼åº¦ 0.6) â†’ [pattern_10]

è·¯å¾„1å¾—åˆ†:
  pattern_5:  0.8 + 0.7 = 1.5
  pattern_10: 0.8 + 0.6 = 1.4
  pattern_20: 0.7 = 0.7
```

---

### è·¯å¾„2: é¢†åŸŸç›¸å…³å¬å› (Idea â†’ Domain â†’ Pattern)

#### å¬å›æµç¨‹
```
ç”¨æˆ· Idea (æ–‡æœ¬)
    â†“ å…³é”®è¯åŒ¹é… Domain name
ç›¸å…³ Domain (Top-5)
    â†“ åå‘æŸ¥æ‰¾ Pattern â†’ Domain è¾¹
åœ¨ Domain ä¸­è¡¨ç°å¥½çš„ Pattern
    â†“ æŒ‰ effectiveness & confidence åŠ æƒ
Top-5 Pattern (å¾—åˆ†å­—å…¸)
```

#### Domain åŒ¹é…é€»è¾‘

**æ–¹æ³•1: å…³é”®è¯åŒ¹é…** (ä¼˜å…ˆ)
```python
domain_scores = []
user_tokens = set(user_idea.lower().split())

for domain in domains:
    domain_name = domain['name']
    domain_tokens = set(domain_name.lower().split())

    # ç®€å•çš„è¯æ±‡é‡å 
    match_score = len(user_tokens & domain_tokens) / max(len(user_tokens), 1)

    if match_score > 0:
        domain_scores.append((domain_id, match_score))

# æ’åºå¹¶å– Top-5
domain_scores.sort(reverse=True)
top_domains = domain_scores[:5]
```

**æ–¹æ³•2: é€šè¿‡ç›¸ä¼¼ Idea çš„ Domain** (å¤‡é€‰)
```python
if not domain_scores:  # å¦‚æœæ²¡æœ‰ç›´æ¥åŒ¹é…
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ Idea
    similarities = [(idea, compute_similarity(user_idea, idea['description']))
                    for idea in ideas]
    top_idea = max(similarities, key=lambda x: x[1])[0]

    # è·å–è¯¥ Idea çš„ Domain (é€šè¿‡ belongs_to è¾¹)
    for successor in G.successors(top_idea['idea_id']):
        edge_data = G[top_idea['idea_id']][successor]
        if edge_data['relation'] == 'belongs_to':
            domain_id = successor
            weight = edge_data['weight']
            domain_scores.append((domain_id, weight))
```

---

#### Pattern å¾—åˆ†è®¡ç®—

**ç®—åˆ†é€»è¾‘**:
```python
pattern_scores = defaultdict(float)

for domain_id, domain_weight in top_5_domains:
    # åå‘æŸ¥æ‰¾: å“ªäº› Pattern åœ¨è¯¥ Domain ä¸­è¡¨ç°å¥½?
    for predecessor in G.predecessors(domain_id):
        edge_data = G[predecessor][domain_id]

        if edge_data['relation'] == 'works_well_in':
            pattern_id = predecessor
            effectiveness = edge_data['effectiveness']  # [-1, 1]
            confidence = edge_data['confidence']  # [0, 1]

            # å¾—åˆ† = Domainç›¸å…³åº¦ Ã— æ•ˆæœ Ã— ç½®ä¿¡åº¦
            # max(effectiveness, 0.1) é¿å…è´Ÿå€¼
            score = domain_weight * max(effectiveness, 0.1) * confidence
            pattern_scores[pattern_id] += score

# æ’åºå¹¶åªä¿ç•™ Top-5 (è¾…åŠ©é€šé“)
sorted_patterns = sorted(pattern_scores.items(), reverse=True)
top_patterns = dict(sorted_patterns[:5])
```

**å…³é”®ç‚¹**:
- `effectiveness` è´Ÿå€¼æ—¶å– 0.1,é¿å…æƒ©ç½šè¿‡åº¦
- æœ€ç»ˆåªä¿ç•™ **Top-5 ä¸ª Pattern** (è·¯å¾„2 æ˜¯è¾…åŠ©é€šé“)

**ç¤ºä¾‹**:
```
ç”¨æˆ· Idea: "ä½¿ç”¨ distillation æŠ€æœ¯è¿›è¡Œè·¨é¢†åŸŸæ–‡æœ¬åˆ†ç±»"

åŒ¹é… Domain:
  domain_2 (Natural Language Processing, ç›¸å…³åº¦=0.25)

åœ¨ domain_2 ä¸­è¡¨ç°å¥½çš„ Pattern:
  pattern_5  (effectiveness=0.12, confidence=0.75)
  pattern_10 (effectiveness=0.08, confidence=0.60)

è·¯å¾„2å¾—åˆ†:
  pattern_5:  0.25 Ã— 0.12 Ã— 0.75 = 0.0225
  pattern_10: 0.25 Ã— 0.10 Ã— 0.60 = 0.0150  (max(0.08, 0.1) = 0.1)
```

---

### è·¯å¾„3: ç›¸ä¼¼ Paper å¬å› (Idea â†’ Paper â†’ Pattern)

#### å¬å›æµç¨‹
```
ç”¨æˆ· Idea (æ–‡æœ¬)
    â†“ [ç²—æ’] Jaccard ç­›é€‰ Top-100 (åŸºäº Paper Title)
å€™é€‰ Paper (100ä¸ª)
    â†“ [ç²¾æ’] Embedding é‡æ’ Top-20 (åŸºäº Paper Title)
ç›¸ä¼¼ Paper (20ä¸ª)
    â†“ æŸ¥æ‰¾ Paper â†’ Pattern è¾¹
Pattern é›†åˆ
    â†“ æŒ‰ similarity Ã— quality åŠ æƒ
Top-10 Pattern (å¾—åˆ†å­—å…¸)
```

**è®¾è®¡ç†å¿µ**:
- **è·¯å¾„1** ä½¿ç”¨ Idea Description è®¡ç®—ç›¸ä¼¼åº¦ â†’ æ•æ‰**æ ¸å¿ƒæ€æƒ³/æ¦‚å¿µ**çš„ç›¸ä¼¼æ€§
- **è·¯å¾„3** ä½¿ç”¨ Paper Title è®¡ç®—ç›¸ä¼¼åº¦ â†’ æ•æ‰**ç ”ç©¶ä¸»é¢˜/å…·ä½“é—®é¢˜**çš„ç›¸ä¼¼æ€§
- ä¸¤è€…äº’è¡¥,é¿å…é‡å¤

#### ä¸¤é˜¶æ®µå¬å›ä¼˜åŒ–

**ç²—æ’é˜¶æ®µ** (Jaccard):
```python
coarse_similarities = []
for paper in papers:  # 8,285 ä¸ª
    paper_title = paper['title']  # ä½¿ç”¨è®ºæ–‡æ ‡é¢˜
    sim = compute_jaccard_similarity(user_idea, paper_title)

    if sim > 0.05:  # é™ä½é˜ˆå€¼ä¿ç•™æ›´å¤šå€™é€‰
        coarse_similarities.append((paper_id, sim))

# æ’åºå¹¶å– Top-100
coarse_similarities.sort(reverse=True)
candidates = coarse_similarities[:100]
```

**ç²¾æ’é˜¶æ®µ** (Embedding):
```python
fine_similarities = []
for paper_id, _ in candidates:  # 100 ä¸ª
    paper = paper_id_to_paper[paper_id]
    paper_title = paper['title']  # ä½¿ç”¨è®ºæ–‡æ ‡é¢˜

    sim = compute_embedding_similarity(user_idea, paper_title)

    if sim > 0.1:  # è¿‡æ»¤ä½ç›¸ä¼¼åº¦
        quality = _get_paper_quality(paper)
        combined_weight = sim * quality
        fine_similarities.append((paper_id, sim, quality, combined_weight))

# æŒ‰ç»¼åˆæƒé‡æ’åºå¹¶å– Top-20
fine_similarities.sort(key=lambda x: x[3], reverse=True)
top_papers = fine_similarities[:20]
```

---

#### Pattern å¾—åˆ†è®¡ç®—

**ç®—åˆ†é€»è¾‘**:
```python
pattern_scores = defaultdict(float)

for paper_id, similarity, paper_quality, combined_weight in top_20_papers:
    # ä»å›¾è°±ä¸­æŸ¥æ‰¾ Paper ä½¿ç”¨çš„ Pattern
    for successor in G.successors(paper_id):
        edge_data = G[paper_id][successor]

        if edge_data['relation'] == 'uses_pattern':
            pattern_id = successor
            pattern_quality = edge_data['quality']  # Paperè´¨é‡

            # å¾—åˆ† = ç›¸ä¼¼åº¦ Ã— Paperè´¨é‡ Ã— Patternè´¨é‡
            score = combined_weight * pattern_quality
            pattern_scores[pattern_id] += score

# æ’åºå¹¶åªä¿ç•™ Top-10
sorted_patterns = sorted(pattern_scores.items(), reverse=True)
top_patterns = dict(sorted_patterns[:10])
```

**å…³é”®ç‚¹**:
- ç»¼åˆè€ƒè™‘ Paper ä¸ç”¨æˆ· Idea çš„ç›¸ä¼¼åº¦ã€Paper è´¨é‡ã€Pattern è´¨é‡
- å¦‚æœå¤šä¸ªç›¸ä¼¼ Paper éƒ½ä½¿ç”¨äº†åŒä¸€ä¸ª Pattern,å¾—åˆ†ä¼š**ç´¯åŠ **
- æœ€ç»ˆåªä¿ç•™ **Top-10 ä¸ª Pattern**

**ç¤ºä¾‹**:
```
ç”¨æˆ· Idea: "ä½¿ç”¨ Transformer è¿›è¡Œæ–‡æœ¬åˆ†ç±»"

ç›¸ä¼¼ Paper:
  Paper_1 (ç›¸ä¼¼åº¦=0.85, è´¨é‡=0.5) â†’ pattern_5 (è´¨é‡=0.5)
  Paper_2 (ç›¸ä¼¼åº¦=0.78, è´¨é‡=0.5) â†’ pattern_5 (è´¨é‡=0.5)
  Paper_3 (ç›¸ä¼¼åº¦=0.72, è´¨é‡=0.5) â†’ pattern_10 (è´¨é‡=0.5)

è·¯å¾„3å¾—åˆ†:
  pattern_5:  (0.85Ã—0.5)Ã—0.5 + (0.78Ã—0.5)Ã—0.5 = 0.2125 + 0.195 = 0.4075
  pattern_10: (0.72Ã—0.5)Ã—0.5 = 0.18
```

---

## å¤šè·¯èåˆä¸ç²¾æ’

### èåˆç­–ç•¥

#### è·¯å¾„æƒé‡
```python
PATH1_WEIGHT = 0.4  # ç›¸ä¼¼ Idea å¬å› (é‡è¦)
PATH2_WEIGHT = 0.2  # é¢†åŸŸç›¸å…³å¬å› (è¾…åŠ©)
PATH3_WEIGHT = 0.4  # ç›¸ä¼¼ Paper å¬å› (é‡è¦)
```

**æƒé‡è®¾è®¡ç†ç”±**:
- **è·¯å¾„1 (0.4)**: ç›´æ¥åˆ©ç”¨å†å²æˆåŠŸç»éªŒ,æœ€å¯é 
- **è·¯å¾„2 (0.2)**: é¢†åŸŸæ³›åŒ–èƒ½åŠ›å¼º,ä½†è¾ƒç²—ç²’åº¦,ä½œä¸ºè¾…åŠ©
- **è·¯å¾„3 (0.4)**: ç»†ç²’åº¦åŒ¹é…,è´¨é‡å¯¼å‘,ä¸è·¯å¾„1åŒç­‰é‡è¦

---

#### æŒ‰ Pattern èšåˆå¾—åˆ†

**èåˆé€»è¾‘**:
```python
# æ”¶é›†ä¸‰è·¯å¬å›çš„æ‰€æœ‰ Pattern
all_patterns = set(path1_scores.keys()) | set(path2_scores.keys()) | set(path3_scores.keys())

# è®¡ç®—æ¯ä¸ª Pattern çš„æœ€ç»ˆå¾—åˆ†
final_scores = {}
for pattern_id in all_patterns:
    score1 = path1_scores.get(pattern_id, 0.0) * PATH1_WEIGHT
    score2 = path2_scores.get(pattern_id, 0.0) * PATH2_WEIGHT
    score3 = path3_scores.get(pattern_id, 0.0) * PATH3_WEIGHT

    final_scores[pattern_id] = score1 + score2 + score3

# æ’åºå¹¶è¿”å› Top-10
ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
top_10 = ranked[:10]
```

**å…³é”®ç‚¹**:
1. **å„è·¯ç‹¬ç«‹ç®—åˆ†**: æ¯æ¡è·¯å¾„ç‹¬ç«‹è®¡ç®— Pattern å¾—åˆ†,äº’ä¸å½±å“
2. **åŠ æƒçº¿æ€§èåˆ**: æŒ‰é¢„å®šä¹‰æƒé‡ç®€å•ç›¸åŠ 
3. **Top-K ç²¾æ’**: æœ€ç»ˆè¿”å›å¾—åˆ†æœ€é«˜çš„ 10 ä¸ª Pattern

---

#### ç»“æœç¤ºä¾‹

**å¬å›ç»“æœæ—¥å¿—**:
```
================================================================================
ğŸ“Š å¬å›ç»“æœ Top-10
================================================================================

ã€Rank 1ã€‘ pattern_111
  åç§°: Reframing Zero-Shot Generalization
  æœ€ç»ˆå¾—åˆ†: 0.6571
  - è·¯å¾„1 (ç›¸ä¼¼Idea):   0.5257 (å æ¯” 80.0%)
  - è·¯å¾„2 (é¢†åŸŸç›¸å…³):   0.0000 (å æ¯” 0.0%)
  - è·¯å¾„3 (ç›¸ä¼¼Paper):  0.1314 (å æ¯” 20.0%)
  èšç±»å¤§å°: 22 ç¯‡è®ºæ–‡
  å½’çº³æ€»ç»“: This cluster explores innovative methods to enhance zero-shot generalization...

ã€Rank 2ã€‘ pattern_110
  åç§°: Reframing Few Shot Learning Robustness
  æœ€ç»ˆå¾—åˆ†: 0.4990
  - è·¯å¾„1 (ç›¸ä¼¼Idea):   0.3036 (å æ¯” 60.8%)
  - è·¯å¾„2 (é¢†åŸŸç›¸å…³):   0.0000 (å æ¯” 0.0%)
  - è·¯å¾„3 (ç›¸ä¼¼Paper):  0.1954 (å æ¯” 39.2%)
  èšç±»å¤§å°: 24 ç¯‡è®ºæ–‡
  å½’çº³æ€»ç»“: This cluster introduces innovative frameworks to enhance few-shot learning...

...
```

**åˆ†æ•°è§£è¯»**:
- `pattern_111` ä¸»è¦ç”±**è·¯å¾„1** (ç›¸ä¼¼ Idea) è´¡çŒ® (80%)
- `pattern_110` åœ¨**è·¯å¾„1** å’Œ**è·¯å¾„3** å‡æœ‰è´¡çŒ®

---

## å½“å‰å±€é™ä¸æ”¹è¿›æ–¹å‘

### 1. Review æ•°æ®ç¼ºå¤±

#### ç°çŠ¶
- âš ï¸ **Paper èŠ‚ç‚¹æš‚æ—  Review è¯„åˆ†æ•°æ®**
- æ‰€æœ‰ Paper è´¨é‡åˆ†é»˜è®¤ä¸º **0.5**
- å¯¼è‡´**è·¯å¾„3** å’Œ **Paperâ†’Pattern è¾¹æƒé‡**å¤±å»è´¨é‡åŒºåˆ†èƒ½åŠ›

#### å½±å“
- **Paperâ†’Pattern è¾¹**: `quality` å…¨éƒ¨ä¸º 0.5,æ— æ³•åæ˜ çœŸå®è®ºæ–‡è´¨é‡
- **è·¯å¾„3 å¬å›**: åªèƒ½åŸºäºç›¸ä¼¼åº¦,æ— æ³•ä¼˜å…ˆæ¨èé«˜è´¨é‡ Paper çš„ Pattern

#### æ”¹è¿›æ–¹æ¡ˆ
```python
# å½“è¡¥å…… Review æ•°æ®å,è´¨é‡è¯„åˆ†å°†è‡ªåŠ¨ç”Ÿæ•ˆ
def _get_paper_quality(paper):
    reviews = paper.get('reviews', [])
    if reviews:
        scores = [r['overall_score'] for r in reviews]
        avg_score = np.mean(scores)
        return (avg_score - 1) / 9  # å½’ä¸€åŒ–åˆ° [0, 1]
    return 0.5  # é»˜è®¤å€¼
```

**ä¸‹ä¸€æ­¥**:
- è¡¥å…… ICLR 2025 çš„ Review æ•°æ®
- é‡æ–°è¿è¡Œ `build_edges.py` æ›´æ–°è¾¹æƒé‡
- å¬å›è´¨é‡å°†è‡ªåŠ¨æå‡,æ— éœ€ä¿®æ”¹ä»£ç 

---

### 2. Domain ç²’åº¦è¿‡ç²—

#### ç°çŠ¶
- 98 ä¸ª Domain,ç²’åº¦è¾ƒå¤§ (å¦‚ "Natural Language Processing" åŒ…å« 1000+ ç¯‡è®ºæ–‡)
- 200+ ä¸ª sub_domains,åˆ†å¸ƒä¸å‡åŒ€
- **è·¯å¾„2** å¬å›æ—¶,Domain åŒ¹é…ä¸å¤Ÿç²¾ç¡®

#### å½±å“
- å…³é”®è¯åŒ¹é…å®¹æ˜“å¤±è´¥ (Domain name é€šå¸¸åªæœ‰ 2-3 ä¸ªè¯)
- Domain å†… Pattern è¿‡å¤š,åŒºåˆ†åº¦ä¸è¶³

#### æ”¹è¿›æ–¹æ¡ˆ

**æ–¹æ¡ˆ1: Domain åˆ†å±‚èšåˆ**
```python
# æ„å»º Domain å±‚çº§ç»“æ„
hierarchy = {
    'Natural Language Processing': {
        'Text Classification': [...],
        'Machine Translation': [...],
        'Question Answering': [...]
    },
    'Computer Vision': {
        '3D Reconstruction': [...],
        'Object Detection': [...]
    }
}

# å¬å›æ—¶å…ˆåŒ¹é…å¤§é¢†åŸŸ,å†åŒ¹é…å­é¢†åŸŸ
main_domain = match_main_domain(user_idea)
sub_domain = match_sub_domain(user_idea, main_domain)
```

**æ–¹æ¡ˆ2: ä½¿ç”¨ sub_domains è¿›è¡Œç²¾ç»†åŒ¹é…**
```python
# æ‰©å±•å…³é”®è¯åŒ¹é…åˆ° sub_domains
for domain in domains:
    all_tokens = set(domain['name'].lower().split())
    all_tokens.update([s.lower() for s in domain['sub_domains']])

    match_score = len(user_tokens & all_tokens) / max(len(user_tokens), 1)
```

**æ–¹æ¡ˆ3: åŸºäº Embedding çš„ Domain æ£€ç´¢**
```python
# ä½¿ç”¨ Embedding è®¡ç®—ç”¨æˆ· Idea ä¸ Domain çš„è¯­ä¹‰ç›¸ä¼¼åº¦
domain_embeddings = precompute_domain_embeddings()  # é¢„è®¡ç®—
user_embedding = get_embedding(user_idea)

similarities = []
for domain_id, domain_emb in domain_embeddings.items():
    sim = cosine_similarity(user_embedding, domain_emb)
    similarities.append((domain_id, sim))

top_domains = sorted(similarities, reverse=True)[:5]
```

---

### 3. Pattern å‘½åä¸æ€»ç»“

#### ç°çŠ¶
- 124 ä¸ª Pattern ä¸­,**912 ä¸ªå·²é€šè¿‡ LLM å¢å¼º**
- å¢å¼ºå†…å®¹åŒ…æ‹¬:
  - `representative_ideas`: ä»£è¡¨æ€§æƒ³æ³•å½’çº³
  - `common_tricks`: å¸¸è§æŠ€å·§åˆ—è¡¨
  - `naming_suggestion`: Pattern å‘½åå»ºè®®

#### é—®é¢˜
- éƒ¨åˆ† Pattern å‘½åå¯èƒ½ä¸å¤Ÿç›´è§‚
- æ€»ç»“å†…å®¹å¯èƒ½éœ€è¦æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µè¿­ä»£ä¼˜åŒ–

#### æ”¹è¿›æ–¹æ¡ˆ
- æ ¹æ®å¬å›æ•ˆæœåé¦ˆ,è°ƒæ•´ Pattern å‘½å
- ä½¿ç”¨æ›´å¼ºçš„ LLM æ¨¡å‹ (å¦‚ GPT-4) é‡æ–°ç”Ÿæˆæ€»ç»“
- è€ƒè™‘åŠ å…¥ç”¨æˆ·åé¦ˆæœºåˆ¶,æŒç»­ä¼˜åŒ– Pattern æè¿°

---

### 4. å¬å›æ•ˆç‡ä¼˜åŒ–

#### ç°çŠ¶
- **ä¸¤é˜¶æ®µå¬å›**: ä» ~7 åˆ†é’Ÿä¼˜åŒ–åˆ° ~27 ç§’ (æé€Ÿ 13 å€)
- ä»æœ‰ä¼˜åŒ–ç©ºé—´

#### è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆ1: Embedding ç¼“å­˜**
```python
# é¢„è®¡ç®—æ‰€æœ‰ Idea å’Œ Paper çš„ Embedding
idea_embeddings = precompute_all_embeddings(ideas)
paper_embeddings = precompute_all_embeddings(papers)

# å¬å›æ—¶ç›´æ¥ä½¿ç”¨ç¼“å­˜
user_embedding = get_embedding(user_idea)
similarities = [cosine_similarity(user_embedding, idea_emb)
                for idea_emb in idea_embeddings]
```

**æ–¹æ¡ˆ2: å‘é‡æ•°æ®åº“**
- ä½¿ç”¨ Faiss/Milvus ç­‰å‘é‡æ•°æ®åº“
- æ”¯æŒé«˜æ•ˆçš„ ANN (è¿‘ä¼¼æœ€è¿‘é‚») æ£€ç´¢
- å¬å›é€Ÿåº¦å¯è¿›ä¸€æ­¥æå‡åˆ° **~1-3 ç§’**

**æ–¹æ¡ˆ3: GPU åŠ é€Ÿ**
- ä½¿ç”¨ GPU æ‰¹é‡è®¡ç®— Embedding ç›¸ä¼¼åº¦
- é€‚åˆå¤§è§„æ¨¡å®æ—¶å¬å›åœºæ™¯

---

### 5. å¤šæ¨¡æ€æ”¯æŒ

#### å½“å‰çŠ¶æ€
- ä»…æ”¯æŒæ–‡æœ¬ Idea å’Œ Paper çš„åŒ¹é…
- æœªåˆ©ç”¨è®ºæ–‡çš„å…¶ä»–ä¿¡æ¯ (å¦‚å›¾è¡¨ã€å…¬å¼ã€ä»£ç ç­‰)

#### æœªæ¥æ‰©å±•
- æ”¯æŒå¤šæ¨¡æ€ Embedding (æ–‡æœ¬ + å›¾åƒ + ä»£ç )
- å¼•å…¥è®ºæ–‡çš„å›¾è¡¨ã€ç®—æ³•ä¼ªä»£ç ç­‰ä½œä¸ºè¾…åŠ©ç‰¹å¾
- æå‡å¬å›çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§

---

### 6. åŠ¨æ€æ›´æ–°æœºåˆ¶

#### å½“å‰çŠ¶æ€
- çŸ¥è¯†å›¾è°±ä¸ºé™æ€æ•°æ®,éœ€æ‰‹åŠ¨é‡æ–°æ„å»º
- æ— æ³•å®æ—¶å¸æ”¶æ–°è®ºæ–‡

#### æ”¹è¿›æ–¹æ¡ˆ
- **å¢é‡æ›´æ–°**: æ”¯æŒæ–°è®ºæ–‡åŠ¨æ€åŠ å…¥å›¾è°±
- **åœ¨çº¿å­¦ä¹ **: æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´ Pattern æƒé‡
- **ç‰ˆæœ¬ç®¡ç†**: æ”¯æŒå›¾è°±çš„ç‰ˆæœ¬å›æ»šå’Œ A/B æµ‹è¯•

---

## æ€»ç»“

### ç³»ç»Ÿäº®ç‚¹
1. âœ… **å®Œæ•´çš„çŸ¥è¯†å›¾è°±**: 16,790 èŠ‚ç‚¹,444,872 æ¡è¾¹,å…¨é¢è¦†ç›– ICLR 2025 æ•°æ®
2. âœ… **ä¸‰è·¯å¬å›ç­–ç•¥**: å…¼é¡¾ç›¸ä¼¼åº¦ã€é¢†åŸŸå’Œè´¨é‡,å¬å›å…¨é¢ä¸”å‡†ç¡®
3. âœ… **ä¸¤é˜¶æ®µä¼˜åŒ–**: æé€Ÿ 13 å€,å®ç°ç§’çº§å¬å›
4. âœ… **LLM å¢å¼º**: 912 ä¸ª Pattern ç»è¿‡ LLM å½’çº³æ€»ç»“,å¯è¯»æ€§å¼º
5. âœ… **å¯æ‰©å±•æ¶æ„**: æ¨¡å—åŒ–è®¾è®¡,æ˜“äºå¢åŠ æ–°æ•°æ®æºå’Œå¬å›è·¯å¾„

### å¾…æ”¹è¿›ç‚¹
1. âš ï¸ **è¡¥å…… Review æ•°æ®**: æå‡è´¨é‡è¯„åˆ†çš„å‡†ç¡®æ€§
2. âš ï¸ **ä¼˜åŒ– Domain åŒ¹é…**: å¼•å…¥å±‚çº§ç»“æ„æˆ– Embedding åŒ¹é…
3. âš ï¸ **å‘é‡æ•°æ®åº“**: è¿›ä¸€æ­¥æå‡å¬å›æ•ˆç‡
4. âš ï¸ **åŠ¨æ€æ›´æ–°**: æ”¯æŒå¢é‡æ›´æ–°å’Œåœ¨çº¿å­¦ä¹ 

---

## é™„å½•

### æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `scripts/build_entity_v3.py` | èŠ‚ç‚¹æ„å»ºè„šæœ¬ |
| `scripts/build_edges.py` | è¾¹æ„å»ºè„šæœ¬ |
| `scripts/recall_system.py` | å¬å›ç³»ç»Ÿå®ç° (ç±»å°è£…ç‰ˆæœ¬) |
| `scripts/simple_recall_demo.py` | å¬å›ç³»ç»Ÿ Demo (å•æµ‹è¯•ç‰ˆæœ¬) |
| `output/nodes_*.json` | èŠ‚ç‚¹æ•°æ®æ–‡ä»¶ |
| `output/edges.json` | è¾¹æ•°æ®æ–‡ä»¶ |
| `output/knowledge_graph_v2.gpickle` | NetworkX å›¾è°±æ–‡ä»¶ |

### å‚æ•°é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `PATH1_TOP_K_IDEAS` | 10 | è·¯å¾„1å¬å›çš„ç›¸ä¼¼ Idea æ•°é‡ |
| `PATH1_FINAL_TOP_K` | 10 | è·¯å¾„1æœ€ç»ˆä¿ç•™çš„ Pattern æ•°é‡ |
| `PATH2_TOP_K_DOMAINS` | 5 | è·¯å¾„2å¬å›çš„ç›¸å…³ Domain æ•°é‡ |
| `PATH2_FINAL_TOP_K` | 5 | è·¯å¾„2æœ€ç»ˆä¿ç•™çš„ Pattern æ•°é‡ |
| `PATH3_TOP_K_PAPERS` | 20 | è·¯å¾„3å¬å›çš„ç›¸ä¼¼ Paper æ•°é‡ |
| `PATH3_FINAL_TOP_K` | 10 | è·¯å¾„3æœ€ç»ˆä¿ç•™çš„ Pattern æ•°é‡ |
| `FINAL_TOP_K` | 10 | æœ€ç»ˆè¿”å›çš„ Pattern æ•°é‡ |
| `COARSE_RECALL_SIZE` | 100 | ç²—æ’å€™é€‰æ•°é‡ |
| `TWO_STAGE_RECALL` | True | æ˜¯å¦å¯ç”¨ä¸¤é˜¶æ®µå¬å› |
| `USE_EMBEDDING` | True | æ˜¯å¦ä½¿ç”¨ Embedding (æ¨è) |

---

**æ–‡æ¡£ç‰ˆæœ¬**: V3.0
**æ›´æ–°æ—¥æœŸ**: 2026-01-22
**ä½œè€…**: Idea2Pattern Team

